{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYI2PmM2whPhSLpZ4NEpvu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-zp8rjegxna","executionInfo":{"status":"ok","timestamp":1719169960998,"user_tz":-330,"elapsed":4757,"user":{"displayName":"Luffy Gear5","userId":"09429710877623930155"}},"outputId":"71bf9630-ff24-4601-abf5-00483612c84d"},"outputs":[{"output_type":"stream","name":"stdout","text":["('I',)\n","('reside',)\n","('in',)\n","('banglore',)\n"]}],"source":["from nltk import ngrams\n","#unigrams\n","sentence='I reside in banglore'\n","n=1\n","unigrams=ngrams(sentence.split(),n)\n","for grams in unigrams:\n","  print(grams)"]},{"cell_type":"code","source":["#bigrams\n","sentence='I reside in banglore'\n","n=2\n","bigrams=ngrams(sentence.split(),n)\n","for grams in bigrams:\n","  print(grams)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7dQ4EWJXg55K","executionInfo":{"status":"ok","timestamp":1719169976952,"user_tz":-330,"elapsed":473,"user":{"displayName":"Luffy Gear5","userId":"09429710877623930155"}},"outputId":"ef6b8e5e-30ed-4043-d76d-1e8ffc7213da"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["('I', 'reside')\n","('reside', 'in')\n","('in', 'banglore')\n"]}]},{"cell_type":"code","source":["\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uth5NcL-hloM","executionInfo":{"status":"ok","timestamp":1719170139170,"user_tz":-330,"elapsed":1136,"user":{"displayName":"Luffy Gear5","userId":"09429710877623930155"}},"outputId":"34a175da-979f-46b0-f90a-3610c22f5719"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["import nltk\n","from nltk.probability import FreqDist\n","from random import choice\n","corpus=\"This is a sample text corpus . You can replace it with your own text for training the language model . This is my model.\"\n","tokens=nltk.tokenize.word_tokenize(corpus)\n","bigrams=ngrams(tokens,2)\n","freq_dist=FreqDist(bigrams)\n","freq_dist"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3iDef_5ng6NO","executionInfo":{"status":"ok","timestamp":1719170142184,"user_tz":-330,"elapsed":447,"user":{"displayName":"Luffy Gear5","userId":"09429710877623930155"}},"outputId":"6476dde6-af8d-4dd4-8e2f-c7eef79cf8e8"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["FreqDist({('This', 'is'): 2, ('model', '.'): 2, ('is', 'a'): 1, ('a', 'sample'): 1, ('sample', 'text'): 1, ('text', 'corpus'): 1, ('corpus', '.'): 1, ('.', 'You'): 1, ('You', 'can'): 1, ('can', 'replace'): 1, ...})"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["#generate text using bigrams\n","def generate_text(start_word,length):\n","  c_word=start_word\n","  generated_text=[c_word]\n","  for _ in range(length-1):\n","    next_words=[word[1] for word in freq_dist if word[0]==c_word]\n","    print(next_words)\n","    if next_words:\n","      c_word=choice(next_words)\n","      generated_text.append(c_word)\n","    else:\n","      break\n","  return ' '.join(generated_text)\n","start_word=\"sample\"\n","generated_text=generate_text(start_word,length=10)\n","print(generated_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ed-r8YVAg6bF","executionInfo":{"status":"ok","timestamp":1719170418390,"user_tz":-330,"elapsed":432,"user":{"displayName":"Luffy Gear5","userId":"09429710877623930155"}},"outputId":"7dbb0d42-caa1-40c7-e2f4-e195c989ebe5"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["['text']\n","['corpus', 'for']\n","['training']\n","['the']\n","['language']\n","['model']\n","['.']\n","['You', 'This']\n","['can']\n","sample text for training the language model . You can\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","sentence = '''ఈ వ్యా సం మహాభారతం సాధారణ వ్యా సం గురంచి. తెలుగులో కవితర యం వ్యా సిన గ్రంథం కొరకు, శ్రర\n","మదంధ్ామహాభారతం చూడండి.\n","వ్యా సుడు చెప్ప గా వినాయకుడు మహాభారతాన్ని వ్యా శాడన్న పురాణ కథనం\n","మహాభారతం హందువులకు ప్ంచమ వేదముగా ప్రగ్ణంచబడే భారత ఇతిహాసము. పురాణ సాహతా చరతరప్ా కారం\n","మహాభారత కావ్ా ము వేద కాలం తరాా త, అనగా సుమారు సామానా శక పూరా ం 4000లో దేవ్నాగ్ర లిపిగ్ల సంసక ృ తం\n","Page | 16\n","భాషలో రచించబడింది.దీన్నన్న వేదవ్యా సుడు చెప్ప గా గ్ణప్తి రచించాడన్న హందువుల నమమ కం. 18 ప్రా ములతో, లక్ష\n","శ్లో కములతో ప్ా ప్ంచము లోన్న అతి పెదద ప్దా కావ్ా ములలో ఒకటి. ఈ మహా కావ్యా న్ని 14వ్ శతాబద ంలో కవితర యముగా\n","పేరు పందిన నని య, తికక న, ఎరరనలు తెలుగు లోకి అనువ్దించారు.'''\n","\n","tokens = word_tokenize(sentence)\n","n=2\n","bigrams=ngrams(tokens,n)\n","freq_dist=FreqDist(bigrams)\n","\n","#generate text using bigrams\n","def generate_text(start_word,length):\n","  c_word=start_word\n","  generated_text=[c_word]\n","  for _ in range(length-1):\n","    next_words=[word[1] for word in freq_dist if word[0]==c_word]\n","    print(next_words)\n","    if next_words:\n","      c_word=choice(next_words)\n","      generated_text.append(c_word)\n","    else:\n","      break\n","  return ' '.join(generated_text)\n","\n","start_word=\"వ్యా సం\"\n","generated_text=generate_text(start_word,length=10)\n","print(generated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"apwU_dLkixtM","executionInfo":{"status":"ok","timestamp":1719170555339,"user_tz":-330,"elapsed":438,"user":{"displayName":"Luffy Gear5","userId":"09429710877623930155"}},"outputId":"28fc6dcb-d56d-4d8f-ac4b-7fe5013745a7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n","వ్యా సం\n"]}]},{"cell_type":"code","source":["# file = open(\"text.txt\", \"r\")\n","# corpus = file.read()\n","# n=2\n","# tokens=nltk.word_tokenize(corpus)\n","# bigrams=ngrams(sentence.split(),n)\n","# freq_dist=FreqDist(bigrams)\n"],"metadata":{"id":"D3lPMBVMjdCo"},"execution_count":null,"outputs":[]}]}